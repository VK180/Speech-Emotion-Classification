# -*- coding: utf-8 -*-
"""predict_emotion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c06FNfy125Aj15y1dsalhKTwizIt71t-
"""

import numpy as np
import librosa
import tensorflow as tf
import pickle
import sys
import os
import glob

# Load model and preprocessing objects
model = tf.keras.models.load_model('emotion_ann_model.h5')
with open('label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)
with open('scaler.pkl', 'rb') as f:
    scaler = pickle.load(f)
with open('top_indices.pkl', 'rb') as f:
    top_indices = pickle.load(f)



# Emotion mapping from RAVDESS documentation
emotion_map = {
    1: 'neutral', 2: 'calm', 3: 'happy', 4: 'sad',
    5: 'angry', 6: 'fearful', 7: 'disgust', 8: 'surprised'
}

def get_true_emotion_from_filename(filename):
    """
    Tries to extract the true emotion from a RAVDESS-style filename.
    Example: 03-01-04-01-01-01-16.wav -> 'sad'
    Returns the emotion string or None if the format is not recognized.
    """
    try:
        base_name = os.path.basename(filename)
        parts = base_name.split('.')[0].split('-')
        if len(parts) >= 3:
            emotion_code = int(parts[2])
            return emotion_map.get(emotion_code)
    except (ValueError, IndexError):
        return None
    return None


def extract_comprehensive_features(file_path):
    try:
        audio, sample_rate = librosa.load(file_path, sr=None) # sr=None to keep original sample rate

        features_list = []

        # 1. MFCCs (40 coefficients)
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
        features_list.append(np.mean(mfccs, axis=1))
        features_list.append(np.std(mfccs, axis=1))

        # 2. Delta MFCCs
        mfccs_delta = librosa.feature.delta(mfccs)
        features_list.append(np.mean(mfccs_delta, axis=1))
        features_list.append(np.std(mfccs_delta, axis=1))

        # 3. Delta-Delta MFCCs
        mfccs_delta2 = librosa.feature.delta(mfccs, order=2)
        features_list.append(np.mean(mfccs_delta2, axis=1))
        features_list.append(np.std(mfccs_delta2, axis=1))

        # 4. Chroma Feature
        chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)
        features_list.append(np.mean(chroma, axis=1))
        features_list.append(np.std(chroma, axis=1))

        # 5. MEL Spectrogram Feature
        mel = librosa.feature.melspectrogram(y=audio, sr=sample_rate)
        mel_db = librosa.power_to_db(mel, ref=np.max) # Convert to dB scale
        features_list.append(np.mean(mel_db, axis=1))
        features_list.append(np.std(mel_db, axis=1))

        # 6. Spectral Contrast Feature
        contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate)
        features_list.append(np.mean(contrast, axis=1))
        features_list.append(np.std(contrast, axis=1))

        # 7. Tonnetz Feature
        # Requires harmonic-percussive source separation for better results
        y_harmonic, y_percussive = librosa.effects.hpss(audio)
        tonnetz = librosa.feature.tonnetz(y=y_harmonic, sr=sample_rate)
        features_list.append(np.mean(tonnetz, axis=1))
        features_list.append(np.std(tonnetz, axis=1))

        # 8. Zero Crossing Rate (ZCR)
        zcr = librosa.feature.zero_crossing_rate(y=audio)
        features_list.append(np.mean(zcr, axis=1))
        features_list.append(np.std(zcr, axis=1))

        # 9. Root Mean Square (RMS) Energy
        rms = librosa.feature.rms(y=audio)
        features_list.append(np.mean(rms, axis=1))
        features_list.append(np.std(rms, axis=1))

        # Concatenate all features into a single vector
        features = np.concatenate(features_list)

        return features
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

def predict_emotion(audio_path):
    """
    Predicts the emotion from a single audio file path.
    """
    features = extract_comprehensive_features(audio_path)
    if features is None:
        return None
    features_selected = features[top_indices]
    features_scaled = scaler.transform([features_selected])
    pred = model.predict(features_scaled, verbose=0) # Set verbose=0 to hide progress bar in batch mode
    emotion = label_encoder.inverse_transform([np.argmax(pred)])
    return emotion[0]


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python predict_emotion.py <audio_file.wav | folder_path>")
        sys.exit(1)

    input_path = sys.argv[1]

    # Case 1: Input is a single .wav file
    if os.path.isfile(input_path) and input_path.lower().endswith('.wav'):
        predicted_emotion = predict_emotion(input_path)
        if predicted_emotion:
            true_emotion = get_true_emotion_from_filename(input_path)
            result_line = f"{os.path.basename(input_path)} -> Predicted: {predicted_emotion}"
            if true_emotion:
                result_line += f" | True: {true_emotion}"
            print(result_line)
        else:
            print(f"Feature extraction failed for {os.path.basename(input_path)}.")

    # Case 2: Input is a directory
    elif os.path.isdir(input_path):
        wav_files = glob.glob(os.path.join(input_path, "*.wav"))
        if not wav_files:
            print(f"No .wav files found in '{input_path}'.")
            sys.exit(1)

        correct_count = 0
        total_labeled_files = 0

        print("--- Batch Prediction Results ---")
        for wav_file in wav_files:
            predicted_emotion = predict_emotion(wav_file)
            if not predicted_emotion:
                print(f"{os.path.basename(wav_file)}: Feature extraction failed.")
                continue

            true_emotion = get_true_emotion_from_filename(wav_file)
            result_line = f"{os.path.basename(wav_file)} -> Predicted: {predicted_emotion}"

            if true_emotion:
                total_labeled_files += 1
                result_line += f" | True: {true_emotion}"
                if predicted_emotion == true_emotion:
                    correct_count += 1
                    result_line += " (Correct)"
                else:
                    result_line += " (Incorrect)"

            print(result_line)

        if total_labeled_files > 0:
            accuracy = (correct_count / total_labeled_files) * 100
            print("\n--- Accuracy Summary ---")
            print(f"Accuracy on labeled files: {accuracy:.2f}% ({correct_count}/{total_labeled_files} correct)")
        else:
            print("\nCould not calculate accuracy. No files with recognizable emotion labels found.")

    else:
        print("Invalid input. Please provide a valid .wav file or a directory.")
        sys.exit(1)

